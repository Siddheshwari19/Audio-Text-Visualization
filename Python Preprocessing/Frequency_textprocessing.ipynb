{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Oig1YxMcEk",
        "outputId": "f89f114b-ebb9-4ea2-b6cb-084115b0efec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stopwords\n",
            "  Downloading stopwords-1.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Installing collected packages: stopwords\n",
            "Successfully installed stopwords-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Frequency of words"
      ],
      "metadata": {
        "id": "jPWYdTNZqTmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stopwordsiso"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV0P9YLUM365",
        "outputId": "4e023f76-fc59-41ba-a3e7-6bfa3ccd76fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stopwordsiso\n",
            "  Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stopwordsiso\n",
            "Successfully installed stopwordsiso-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LZzXc2BL88O",
        "outputId": "eba2c529-d30c-4745-f54e-67e4987ea4b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('people', 18), ('guns', 14), ('nt', 13), ('laws', 10), ('lot', 8), ('gun', 8), ('question', 7), ('red', 6), ('flag', 6), ('violence', 5), ('United', 4), ('America', 4), ('law', 4), ('firearms', 4), ('situations', 3), ('criminal', 3), ('curious', 3), ('violent', 3), ('legislation', 3), ('trafficking', 3)]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "from stopwordsiso import stopwords as STOPWORDS\n",
        "import string\n",
        "\n",
        "# Load the spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Get the English stopwords\n",
        "stopwords = STOPWORDS(\"en\")\n",
        "\n",
        "# Read in the transcript text file\n",
        "with open(\"RedFlagLaws.txt\", \"r\") as f:\n",
        "  transcript_text = f.read()\n",
        "\n",
        "# Remove punctuation and digits using string module\n",
        "transcript_text = transcript_text.translate(str.maketrans('', '', string.punctuation+string.digits))\n",
        "\n",
        "# Use spaCy to tokenize the words\n",
        "doc = nlp(transcript_text)\n",
        "tokenized_words = [token.text for token in doc]\n",
        "\n",
        "# Remove stopwords from the tokenized words\n",
        "cleaned_words = [word for word in tokenized_words if word.lower() not in stopwords]\n",
        "\n",
        "# # Remove duplicates from the cleaned words\n",
        "# cleaned_words = list(set(cleaned_words))\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(cleaned_words)\n",
        "\n",
        "# Print the top 20 most frequent words\n",
        "print(word_freq.most_common(20))\n",
        "\n",
        "# Print the final list of cleaned words\n",
        "# print(cleaned_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[('nt', 19), ('background', 11), ('firearm', 10), ('people', 8), ('health', 7), ('check', 7), ('kids', 6), \n",
        "('gun', 6), ('person', 6), ('mental', 6), ('school', 5), ('safe', 5), ('commit', 5), ('crime', 5), ('firearms', 4), \n",
        "('license', 4), ('purchase', 4), ('goal', 4), ('care', 3), ('sense', 3), ('feel', 3), ('community', 3), \n",
        "('shootings', 3), ('dying', 3), ('bring', 3), ('universal', 3), ('checks', 3), ('boyfriend', 3), \n",
        "('girlfriend', 3), ('children', 2), ('empower', 2), ('difficult', 2), ('safety', 2), ('address', 2), \n",
        "('federal', 2), ('change', 2), ('happened', 2), ('drills', 2), ('schools', 2), ('mass', 2)]"
      ],
      "metadata": {
        "id": "_a3rAU8DoXFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trial json create"
      ],
      "metadata": {
        "id": "p9fKOe81qdVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Go4oqf9mL88U"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Create a list of phrases and their related phrases\n",
        "phrases = {\n",
        "    \"More protection for our children\": [\"police are their friends\", \"education empowers\"],\n",
        "    \"Police are their friends\": [\"protect children\"],\n",
        "    \"Education empowers\": [\"take care of themselves and each other\"],\n",
        "    \"Trained and raised to take care of themselves\": [\"could have been a victim\"],\n",
        "    \"Mental health is important\": [\"empower children's minds\"],\n",
        "    \"Address the needs of each community\": [\"invest in people, invest in our kids and their wellness\"],\n",
        "    \"Invest in our kids and their wellness\": [\"seek help\", \"normalize seeking help\"],\n",
        "    \"Universal background checks\": [\"background check system saves lives\"],\n",
        "    \"Background check system saves lives\": [\"avoid tragedies\"]\n",
        "}\n",
        "\n",
        "# Create a dictionary with the phrases as nodes and their related phrases as edges\n",
        "graph = {\"nodes\": [], \"edges\": []}\n",
        "\n",
        "for phrase in phrases:\n",
        "    # Add nodes to the graph\n",
        "    graph[\"nodes\"].append({\"id\": phrase})\n",
        "    for related_phrase in phrases[phrase]:\n",
        "        graph[\"nodes\"].append({\"id\": related_phrase})\n",
        "\n",
        "        # Add edges to the graph\n",
        "        graph[\"edges\"].append({\"source\": phrase, \"target\": related_phrase})\n",
        "\n",
        "# Write the graph to a JSON file\n",
        "with open(\"phrase_net.json\", \"w\") as f:\n",
        "    json.dump(graph, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#json file but with source and target as numbers"
      ],
      "metadata": {
        "id": "5wgr5-xHqoCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwqOaxMTcz5g",
        "outputId": "37019c92-184d-4001-cd71-b1551009d826"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rake-nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I2jtPpqjuVx",
        "outputId": "f790eeb1-d5ac-4997-cc76-804746c6787d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rake-nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake-nltk) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.65.0)\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Read the text file\n",
        "with open('RedFlagLaws.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Clean the text\n",
        "text = text.lower().replace('\\n', ' ').replace('\\r', '').replace('\\t', '')\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "# Stem the tokens\n",
        "ps = PorterStemmer()\n",
        "stemmed_tokens = [ps.stem(w) for w in filtered_tokens]\n",
        "\n",
        "# Identify key phrases\n",
        "# For example, using RAKE\n",
        "from rake_nltk import Rake\n",
        "r = Rake()\n",
        "r.extract_keywords_from_text(text)\n",
        "key_phrases = r.get_ranked_phrases()\n",
        "\n",
        "# Create nodes and links\n",
        "nodes = list(set(key_phrases))\n",
        "links = []\n",
        "for i in range(len(nodes)):\n",
        "    for j in range(i + 1, len(nodes)):\n",
        "        if nodes[i] in nodes[j] or nodes[j] in nodes[i]:\n",
        "            links.append({\"source\": i, \"target\": j, \"value\": 1})\n",
        "\n",
        "# Convert to JSON format\n",
        "json_data = {\"nodes\": [{\"id\": node} for node in nodes], \"links\": links}\n",
        "json.dump(json_data, open('RedFlagLaws_count.json', 'w'))\n"
      ],
      "metadata": {
        "id": "ZNe5zKWxPMqj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFHJLGQVP-rA",
        "outputId": "3032faf0-e1d2-4298-f9e7-eba9ab690e77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'nodes': [{'id': 'declined'}, {'id': 'want'}, {'id': 'poor people'}, {'id': 'solving'}, {'id': 'understand'}, {'id': 'black'}, {'id': 'part'}, {'id': 'seen'}, {'id': 'drugs'}, {'id': 'us'}, {'id': 'shot'}, {'id': 'tough'}, {'id': 'attorney'}, {'id': 'actually preventing violence'}, {'id': 'criminal legal system'}, {'id': 'someone probably'}, {'id': 'law enforcement world'}, {'id': 'keep putting'}, {'id': 'system'}, {'id': 'type'}, {'id': 'personally think'}, {'id': 'better months'}, {'id': 'passing'}, {'id': 'left'}, {'id': 'confrontation'}, {'id': 'laws'}, {'id': 'interactions'}, {'id': 'go'}, {'id': 'might'}, {'id': 'really'}, {'id': 'think'}, {'id': 'fact'}, {'id': 'america actually enforce'}, {'id': 'law enforcement five times'}, {'id': 'look like'}, {'id': 'well thought'}, {'id': 'people never get'}, {'id': 'enforcing'}, {'id': 'especially'}, {'id': 'red flag laws exist'}, {'id': 'pass'}, {'id': 'drug trafficking'}, {'id': 'society'}, {'id': 'case'}, {'id': 'never owned'}, {'id': 'process'}, {'id': 'government'}, {'id': 'really see'}, {'id': 'doubt'}, {'id': 'need'}, {'id': 'believe'}, {'id': 'rights restored'}, {'id': 'much'}, {'id': 'life'}, {'id': 'three investigators'}, {'id': 'people'}, {'id': 'doctor'}, {'id': 'mean'}, {'id': 'provide'}, {'id': 'incarcerate'}, {'id': 'drawing'}, {'id': 'backs'}, {'id': 'certain situations'}, {'id': 'gun anymore'}, {'id': 'hospital'}, {'id': 'caught'}, {'id': 'white militiamen'}, {'id': 'way'}, {'id': 'adds'}, {'id': 'one'}, {'id': 'goes'}, {'id': 'couple'}, {'id': 'audacity'}, {'id': 'carry'}, {'id': 'police officers'}, {'id': 'firearms'}, {'id': 'curious'}, {'id': 'training'}, {'id': 'highest level incarceration'}, {'id': 'labeling criminal'}, {'id': 'firearms away'}, {'id': 'things'}, {'id': 'nurse'}, {'id': 'uniform police officers'}, {'id': 'touched'}, {'id': 'playing'}, {'id': 'month'}, {'id': 'hey'}, {'id': 'gun'}, {'id': 'trying'}, {'id': 'red flag instances'}, {'id': 'red flag laws'}, {'id': 'much whether'}, {'id': 'make sure bias'}, {'id': 'guns'}, {'id': 'therapy'}, {'id': 'talk'}, {'id': 'onus'}, {'id': 'answer'}, {'id': 'implemented'}, {'id': 'incredible danger'}, {'id': 'something'}, {'id': 'knowing'}, {'id': 'overarching question needs'}, {'id': 'think plenty'}, {'id': 'us already'}, {'id': 'situation'}, {'id': 'happen every time'}, {'id': 'violence offense'}, {'id': 'country'}, {'id': 'responsibility'}, {'id': 'needs'}, {'id': 'police normally'}, {'id': 'serve'}, {'id': 'carefully'}, {'id': 'red flag law'}, {'id': 'times'}, {'id': 'okay'}, {'id': 'oh'}, {'id': 'give'}, {'id': 'biases'}, {'id': 'asked'}, {'id': 'characteristics'}, {'id': 'oath keepers'}, {'id': 'regulation'}, {'id': 'lot'}, {'id': 'hinge'}, {'id': 'accountability'}, {'id': 'certain people'}, {'id': 'firearms right'}, {'id': 'probably'}, {'id': 'deadliest'}, {'id': 'brown people'}, {'id': 'executed'}, {'id': 'firearms back'}, {'id': 'law'}, {'id': 'remove'}, {'id': 'goal'}, {'id': 'called'}, {'id': 'certainly think plenty'}, {'id': 'communities every day'}, {'id': 'save lives'}, {'id': 'bills'}, {'id': 'think violent racists'}, {'id': 'elitist people'}, {'id': 'communities'}, {'id': 'gun laws'}, {'id': 'violence'}, {'id': 'firearm'}, {'id': 'balances'}, {'id': 'louisiana would'}, {'id': 'someone else'}, {'id': 'mind'}, {'id': 'situations'}, {'id': 'fix'}, {'id': 'really think'}, {'id': 'mental health professional'}, {'id': 'black men'}, {'id': 'brown men'}, {'id': 'surgeon'}, {'id': 'easily labeled people criminal'}, {'id': 'ideology'}, {'id': 'become'}, {'id': 'proud boys'}, {'id': 'systemic racism'}, {'id': 'vulnerable'}, {'id': 'owning'}, {'id': 'right answer'}, {'id': 'theory'}, {'id': 'resources'}, {'id': 'america criminalizes black men'}, {'id': 'legislation'}, {'id': 'violent person'}, {'id': 'threshold'}, {'id': 'meet'}, {'id': 'hurt'}, {'id': 'focused'}, {'id': 'people need'}, {'id': 'guns drawn'}, {'id': 'critical'}, {'id': 'think violent misogynists'}, {'id': 'label'}, {'id': 'end'}, {'id': 'right wing ideologues'}, {'id': 'met'}, {'id': 'somebody'}, {'id': 'pretty pro red flag laws'}, {'id': 'tools'}, {'id': 'question'}, {'id': 'arrived'}, {'id': 'going forward'}, {'id': 'profiting'}, {'id': 'forever'}, {'id': 'three percenters'}, {'id': 'well'}, {'id': 'figure'}, {'id': 'could avoid'}, {'id': 'though'}, {'id': 'access'}, {'id': 'highest level'}, {'id': 'saving'}, {'id': 'never'}, {'id': 'checks'}, {'id': 'impacting'}, {'id': 'right'}, {'id': 'safest city'}, {'id': 'violently anti trans people'}, {'id': 'day'}, {'id': 'guns taken away'}, {'id': 'gun violence'}, {'id': 'protections'}, {'id': 'america'}, {'id': 'mental health'}, {'id': 'america truly take away guns'}, {'id': 'identify'}, {'id': 'adult'}, {'id': 'keep'}, {'id': 'feel like'}, {'id': 'united states'}, {'id': 'solve crimes'}, {'id': 'sounds great'}, {'id': 'one person'}, {'id': 'authority'}, {'id': 'one state'}, {'id': 'head'}, {'id': 'crime'}, {'id': 'biased'}, {'id': 'rules portion'}, {'id': 'going'}, {'id': 'say'}, {'id': 'judge'}, {'id': 'concerned'}, {'id': 'reality'}, {'id': 'think everyone'}, {'id': 'agree'}, {'id': 'decision'}, {'id': 'leads'}, {'id': 'protected'}, {'id': 'push forward laws'}, {'id': 'yeah'}, {'id': 'enforced'}, {'id': 'racial ideologies'}, {'id': 'owned guns'}, {'id': 'take'}, {'id': 'lack'}, {'id': 'domestic violence history'}, {'id': 'people would agree'}, {'id': 'able'}, {'id': 'risk'}, {'id': 'comes'}, {'id': 'sex trafficking'}, {'id': 'criminalized'}], 'links': [{'source': 2, 'target': 55, 'value': 1}, {'source': 5, 'target': 157, 'value': 1}, {'source': 5, 'target': 170, 'value': 1}, {'source': 5, 'target': 244, 'value': 1}, {'source': 9, 'target': 76, 'value': 1}, {'source': 9, 'target': 97, 'value': 1}, {'source': 9, 'target': 105, 'value': 1}, {'source': 9, 'target': 176, 'value': 1}, {'source': 9, 'target': 238, 'value': 1}, {'source': 13, 'target': 147, 'value': 1}, {'source': 14, 'target': 18, 'value': 1}, {'source': 15, 'target': 69, 'value': 1}, {'source': 15, 'target': 130, 'value': 1}, {'source': 16, 'target': 135, 'value': 1}, {'source': 17, 'target': 216, 'value': 1}, {'source': 18, 'target': 164, 'value': 1}, {'source': 20, 'target': 30, 'value': 1}, {'source': 21, 'target': 86, 'value': 1}, {'source': 22, 'target': 40, 'value': 1}, {'source': 25, 'target': 39, 'value': 1}, {'source': 25, 'target': 91, 'value': 1}, {'source': 25, 'target': 135, 'value': 1}, {'source': 25, 'target': 146, 'value': 1}, {'source': 25, 'target': 186, 'value': 1}, {'source': 25, 'target': 238, 'value': 1}, {'source': 27, 'target': 46, 'value': 1}, {'source': 27, 'target': 70, 'value': 1}, {'source': 27, 'target': 137, 'value': 1}, {'source': 27, 'target': 190, 'value': 1}, {'source': 27, 'target': 228, 'value': 1}, {'source': 29, 'target': 47, 'value': 1}, {'source': 29, 'target': 155, 'value': 1}, {'source': 30, 'target': 104, 'value': 1}, {'source': 30, 'target': 139, 'value': 1}, {'source': 30, 'target': 143, 'value': 1}, {'source': 30, 'target': 155, 'value': 1}, {'source': 30, 'target': 180, 'value': 1}, {'source': 30, 'target': 233, 'value': 1}, {'source': 32, 'target': 211, 'value': 1}, {'source': 33, 'target': 116, 'value': 1}, {'source': 33, 'target': 135, 'value': 1}, {'source': 35, 'target': 194, 'value': 1}, {'source': 35, 'target': 197, 'value': 1}, {'source': 36, 'target': 55, 'value': 1}, {'source': 36, 'target': 201, 'value': 1}, {'source': 39, 'target': 91, 'value': 1}, {'source': 39, 'target': 115, 'value': 1}, {'source': 39, 'target': 135, 'value': 1}, {'source': 44, 'target': 201, 'value': 1}, {'source': 49, 'target': 103, 'value': 1}, {'source': 49, 'target': 111, 'value': 1}, {'source': 49, 'target': 177, 'value': 1}, {'source': 51, 'target': 204, 'value': 1}, {'source': 52, 'target': 92, 'value': 1}, {'source': 55, 'target': 128, 'value': 1}, {'source': 55, 'target': 132, 'value': 1}, {'source': 55, 'target': 144, 'value': 1}, {'source': 55, 'target': 160, 'value': 1}, {'source': 55, 'target': 177, 'value': 1}, {'source': 55, 'target': 206, 'value': 1}, {'source': 55, 'target': 246, 'value': 1}, {'source': 62, 'target': 106, 'value': 1}, {'source': 62, 'target': 153, 'value': 1}, {'source': 63, 'target': 88, 'value': 1}, {'source': 67, 'target': 80, 'value': 1}, {'source': 67, 'target': 208, 'value': 1}, {'source': 67, 'target': 213, 'value': 1}, {'source': 69, 'target': 151, 'value': 1}, {'source': 69, 'target': 221, 'value': 1}, {'source': 69, 'target': 223, 'value': 1}, {'source': 69, 'target': 233, 'value': 1}, {'source': 74, 'target': 83, 'value': 1}, {'source': 75, 'target': 80, 'value': 1}, {'source': 75, 'target': 129, 'value': 1}, {'source': 75, 'target': 134, 'value': 1}, {'source': 75, 'target': 148, 'value': 1}, {'source': 78, 'target': 199, 'value': 1}, {'source': 79, 'target': 181, 'value': 1}, {'source': 80, 'target': 148, 'value': 1}, {'source': 88, 'target': 94, 'value': 1}, {'source': 88, 'target': 146, 'value': 1}, {'source': 88, 'target': 178, 'value': 1}, {'source': 88, 'target': 208, 'value': 1}, {'source': 88, 'target': 209, 'value': 1}, {'source': 88, 'target': 213, 'value': 1}, {'source': 88, 'target': 242, 'value': 1}, {'source': 91, 'target': 115, 'value': 1}, {'source': 91, 'target': 135, 'value': 1}, {'source': 91, 'target': 186, 'value': 1}, {'source': 94, 'target': 178, 'value': 1}, {'source': 94, 'target': 208, 'value': 1}, {'source': 94, 'target': 213, 'value': 1}, {'source': 94, 'target': 242, 'value': 1}, {'source': 98, 'target': 167, 'value': 1}, {'source': 101, 'target': 184, 'value': 1}, {'source': 103, 'target': 111, 'value': 1}, {'source': 103, 'target': 188, 'value': 1}, {'source': 104, 'target': 139, 'value': 1}, {'source': 106, 'target': 153, 'value': 1}, {'source': 108, 'target': 147, 'value': 1}, {'source': 115, 'target': 135, 'value': 1}, {'source': 115, 'target': 186, 'value': 1}, {'source': 123, 'target': 216, 'value': 1}, {'source': 129, 'target': 148, 'value': 1}, {'source': 129, 'target': 204, 'value': 1}, {'source': 134, 'target': 148, 'value': 1}, {'source': 135, 'target': 146, 'value': 1}, {'source': 135, 'target': 186, 'value': 1}, {'source': 135, 'target': 238, 'value': 1}, {'source': 140, 'target': 145, 'value': 1}, {'source': 140, 'target': 207, 'value': 1}, {'source': 147, 'target': 209, 'value': 1}, {'source': 147, 'target': 245, 'value': 1}, {'source': 156, 'target': 212, 'value': 1}, {'source': 157, 'target': 170, 'value': 1}, {'source': 157, 'target': 244, 'value': 1}, {'source': 160, 'target': 181, 'value': 1}, {'source': 165, 'target': 247, 'value': 1}, {'source': 167, 'target': 204, 'value': 1}, {'source': 170, 'target': 211, 'value': 1}, {'source': 170, 'target': 244, 'value': 1}, {'source': 183, 'target': 204, 'value': 1}, {'source': 190, 'target': 228, 'value': 1}, {'source': 208, 'target': 243, 'value': 1}, {'source': 211, 'target': 213, 'value': 1}, {'source': 213, 'target': 243, 'value': 1}, {'source': 219, 'target': 225, 'value': 1}, {'source': 234, 'target': 246, 'value': 1}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to convert source target values to phrases"
      ],
      "metadata": {
        "id": "bK_Pnq2jq-8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the old json data\n",
        "with open('RestrictionOnSpecificGuns.json') as f:\n",
        "    old_data = json.load(f)\n",
        "\n",
        "# Create a dictionary mapping node IDs to their index in the old_data['nodes'] list\n",
        "node_index = {node['id']: index for index, node in enumerate(old_data['nodes'])}\n",
        "\n",
        "# Create a new list of nodes in the desired format\n",
        "new_nodes = [{'id': node['id']} for node in old_data['nodes']]\n",
        "\n",
        "# Create a new list of links in the desired format\n",
        "new_links = [\n",
        "    {\n",
        "        'source': old_data['nodes'][link['source']]['id'],\n",
        "        'target': old_data['nodes'][link['target']]['id'],\n",
        "    }\n",
        "    for link in old_data['links']\n",
        "]\n",
        "\n",
        "# Create a new dictionary with the nodes and links in the desired format\n",
        "new_data = {\n",
        "    'nodes': new_nodes,\n",
        "    'links': new_links\n",
        "}\n",
        "\n",
        "with open('RestrictionOnSpecificGuns.json', 'w') as f:\n",
        "    json.dump(new_data, f, indent=4)"
      ],
      "metadata": {
        "id": "AwDqV7S_j-kI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtEH8y3hVmUc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}